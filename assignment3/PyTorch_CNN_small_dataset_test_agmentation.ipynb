{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "data_train = dset.SVHN('./data/', transform=transforms.Compose([transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                                               std=[0.20,0.20,0.20])]))\n",
    "data_test = dset.SVHN('./data/', split='test', transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                                               std=[0.20,0.20,0.20])\n",
    "                       ]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "want_size = 1000\n",
    "\n",
    "data_size = data_train.data.shape[0]\n",
    "validation_split = .2\n",
    "split = int(np.floor(validation_split * want_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)\n",
    "indices = indices[:want_size]\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, scheduler, num_epochs):\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        scheduler.step()\n",
    "        model.train() # Enter train mode\n",
    "\n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            prediction = model(x)\n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "\n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "\n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        print(\"epoch %d Average loss: %f, Train accuracy: %f, val accuracy: %f\" %\n",
    "                  (epoch+1, ave_loss, train_accuracy, val_accuracy))\n",
    "        #if train_accuracy < 0.15:\n",
    "        #    return loss_history, train_history, val_history\n",
    "\n",
    "    return loss_history, train_history, val_history\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "\n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    for i_step, (x, y) in enumerate(loader):\n",
    "        # x_gpu = x.to(device)\n",
    "        # y_gpu = y.to(device)\n",
    "        prediction = model(x)\n",
    "        _, indices = torch.max(prediction, 1)\n",
    "        correct_samples += torch.sum(indices == y)\n",
    "        total_samples += y.shape[0]\n",
    "\n",
    "    train_accuracy = float(correct_samples) / total_samples\n",
    "    return train_accuracy\n",
    "\n",
    "#loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# We'll use a special helper module to shape it into a flat tensor\n",
    "class Flattener(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size, *_ = x.shape\n",
    "        return x.view(batch_size, -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "Hyperparams = namedtuple(\"Hyperparams\", ['transforms'])\n",
    "RunResult = namedtuple(\"RunResult\", ['val_history', 'final_val_accuracy'])\n",
    "run_record = {}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ColorJitter(brightness=None, contrast=None, saturation=[0.8, 1.2], hue=[-0.2, 0.2])\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    RandomAdjustSharpness(sharpness_factor=3,p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.43, 0.44, 0.47], std=[0.2, 0.2, 0.2])\n",
      ")\n",
      "epoch 1 Average loss: 2.521544, Train accuracy: 0.167500, val accuracy: 0.250000\n",
      "epoch 2 Average loss: 2.080314, Train accuracy: 0.358750, val accuracy: 0.325000\n",
      "epoch 3 Average loss: 1.636829, Train accuracy: 0.498750, val accuracy: 0.520000\n",
      "epoch 4 Average loss: 1.206157, Train accuracy: 0.640000, val accuracy: 0.530000\n",
      "epoch 5 Average loss: 0.986299, Train accuracy: 0.726250, val accuracy: 0.600000\n",
      "epoch 6 Average loss: 0.762237, Train accuracy: 0.796250, val accuracy: 0.645000\n",
      "epoch 7 Average loss: 0.573624, Train accuracy: 0.838750, val accuracy: 0.605000\n",
      "epoch 8 Average loss: 0.444745, Train accuracy: 0.896250, val accuracy: 0.675000\n",
      "epoch 9 Average loss: 0.351998, Train accuracy: 0.917500, val accuracy: 0.675000\n",
      "epoch 10 Average loss: 0.300495, Train accuracy: 0.933750, val accuracy: 0.675000\n",
      "epoch 11 Average loss: 0.254616, Train accuracy: 0.943750, val accuracy: 0.685000\n",
      "epoch 12 Average loss: 0.212357, Train accuracy: 0.968750, val accuracy: 0.680000\n",
      "epoch 13 Average loss: 0.159931, Train accuracy: 0.982500, val accuracy: 0.690000\n",
      "epoch 14 Average loss: 0.146300, Train accuracy: 0.982500, val accuracy: 0.685000\n",
      "epoch 15 Average loss: 0.148950, Train accuracy: 0.978750, val accuracy: 0.700000\n",
      "epoch 16 Average loss: 0.144630, Train accuracy: 0.982500, val accuracy: 0.695000\n",
      "epoch 17 Average loss: 0.123054, Train accuracy: 0.986250, val accuracy: 0.690000\n",
      "epoch 18 Average loss: 0.131046, Train accuracy: 0.983750, val accuracy: 0.695000\n",
      "epoch 19 Average loss: 0.109875, Train accuracy: 0.991250, val accuracy: 0.695000\n",
      "epoch 20 Average loss: 0.105539, Train accuracy: 0.992500, val accuracy: 0.690000\n",
      "Compose(\n",
      "    ColorJitter(brightness=None, contrast=None, saturation=[0.8, 1.2], hue=[-0.2, 0.2])\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    RandomAdjustSharpness(sharpness_factor=3,p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.43, 0.44, 0.47], std=[0.2, 0.2, 0.2])\n",
      ")\n",
      "epoch 1 Average loss: 2.582625, Train accuracy: 0.145000, val accuracy: 0.175000\n",
      "epoch 2 Average loss: 2.201963, Train accuracy: 0.282500, val accuracy: 0.240000\n",
      "epoch 3 Average loss: 1.925332, Train accuracy: 0.391250, val accuracy: 0.415000\n",
      "epoch 4 Average loss: 1.561238, Train accuracy: 0.521250, val accuracy: 0.425000\n",
      "epoch 5 Average loss: 1.242348, Train accuracy: 0.631250, val accuracy: 0.555000\n",
      "epoch 6 Average loss: 1.007720, Train accuracy: 0.697500, val accuracy: 0.595000\n",
      "epoch 7 Average loss: 0.842729, Train accuracy: 0.756250, val accuracy: 0.595000\n",
      "epoch 8 Average loss: 0.663465, Train accuracy: 0.811250, val accuracy: 0.640000\n",
      "epoch 9 Average loss: 0.605176, Train accuracy: 0.833750, val accuracy: 0.595000\n",
      "epoch 10 Average loss: 0.503693, Train accuracy: 0.870000, val accuracy: 0.605000\n",
      "epoch 11 Average loss: 0.417779, Train accuracy: 0.905000, val accuracy: 0.630000\n",
      "epoch 12 Average loss: 0.345021, Train accuracy: 0.926250, val accuracy: 0.625000\n",
      "epoch 13 Average loss: 0.298205, Train accuracy: 0.942500, val accuracy: 0.620000\n",
      "epoch 14 Average loss: 0.262934, Train accuracy: 0.945000, val accuracy: 0.620000\n",
      "epoch 15 Average loss: 0.236981, Train accuracy: 0.953750, val accuracy: 0.620000\n",
      "epoch 16 Average loss: 0.224166, Train accuracy: 0.960000, val accuracy: 0.620000\n",
      "epoch 17 Average loss: 0.221123, Train accuracy: 0.962500, val accuracy: 0.625000\n",
      "epoch 18 Average loss: 0.199127, Train accuracy: 0.962500, val accuracy: 0.620000\n",
      "epoch 19 Average loss: 0.204626, Train accuracy: 0.963750, val accuracy: 0.630000\n",
      "epoch 20 Average loss: 0.188402, Train accuracy: 0.972500, val accuracy: 0.625000\n",
      "Compose(\n",
      "    ColorJitter(brightness=None, contrast=None, saturation=[0.8, 1.2], hue=[-0.2, 0.2])\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    RandomAdjustSharpness(sharpness_factor=3,p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.43, 0.44, 0.47], std=[0.2, 0.2, 0.2])\n",
      ")\n",
      "epoch 1 Average loss: 2.630712, Train accuracy: 0.122500, val accuracy: 0.155000\n",
      "epoch 2 Average loss: 2.372497, Train accuracy: 0.240000, val accuracy: 0.220000\n",
      "epoch 3 Average loss: 2.132037, Train accuracy: 0.316250, val accuracy: 0.300000\n",
      "epoch 4 Average loss: 1.841528, Train accuracy: 0.416250, val accuracy: 0.405000\n",
      "epoch 5 Average loss: 1.561304, Train accuracy: 0.518750, val accuracy: 0.510000\n",
      "epoch 6 Average loss: 1.330454, Train accuracy: 0.605000, val accuracy: 0.530000\n",
      "epoch 7 Average loss: 1.079211, Train accuracy: 0.692500, val accuracy: 0.545000\n",
      "epoch 8 Average loss: 0.899688, Train accuracy: 0.750000, val accuracy: 0.630000\n",
      "epoch 9 Average loss: 0.766092, Train accuracy: 0.795000, val accuracy: 0.610000\n",
      "epoch 10 Average loss: 0.692806, Train accuracy: 0.822500, val accuracy: 0.635000\n",
      "epoch 11 Average loss: 0.592450, Train accuracy: 0.861250, val accuracy: 0.605000\n",
      "epoch 12 Average loss: 0.529451, Train accuracy: 0.878750, val accuracy: 0.630000\n",
      "epoch 13 Average loss: 0.437737, Train accuracy: 0.907500, val accuracy: 0.660000\n",
      "epoch 14 Average loss: 0.411410, Train accuracy: 0.911250, val accuracy: 0.650000\n",
      "epoch 15 Average loss: 0.358454, Train accuracy: 0.923750, val accuracy: 0.640000\n",
      "epoch 16 Average loss: 0.342057, Train accuracy: 0.933750, val accuracy: 0.670000\n",
      "epoch 17 Average loss: 0.307166, Train accuracy: 0.947500, val accuracy: 0.650000\n",
      "epoch 18 Average loss: 0.287419, Train accuracy: 0.953750, val accuracy: 0.650000\n",
      "epoch 19 Average loss: 0.275637, Train accuracy: 0.953750, val accuracy: 0.660000\n",
      "epoch 20 Average loss: 0.278511, Train accuracy: 0.956250, val accuracy: 0.645000\n",
      "Compose(\n",
      "    ColorJitter(brightness=None, contrast=None, saturation=[0.8, 1.2], hue=[-0.2, 0.2])\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    RandomAdjustSharpness(sharpness_factor=3,p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.43, 0.44, 0.47], std=[0.2, 0.2, 0.2])\n",
      ")\n",
      "epoch 1 Average loss: 2.553065, Train accuracy: 0.153750, val accuracy: 0.145000\n",
      "epoch 2 Average loss: 2.219445, Train accuracy: 0.297500, val accuracy: 0.285000\n",
      "epoch 3 Average loss: 1.785275, Train accuracy: 0.457500, val accuracy: 0.425000\n",
      "epoch 4 Average loss: 1.382129, Train accuracy: 0.621250, val accuracy: 0.540000\n",
      "epoch 5 Average loss: 1.060051, Train accuracy: 0.716250, val accuracy: 0.570000\n",
      "epoch 6 Average loss: 0.833564, Train accuracy: 0.776250, val accuracy: 0.635000\n",
      "epoch 7 Average loss: 0.651449, Train accuracy: 0.831250, val accuracy: 0.645000\n",
      "epoch 8 Average loss: 0.475618, Train accuracy: 0.890000, val accuracy: 0.650000\n",
      "epoch 9 Average loss: 0.398461, Train accuracy: 0.912500, val accuracy: 0.670000\n",
      "epoch 10 Average loss: 0.319163, Train accuracy: 0.933750, val accuracy: 0.670000\n",
      "epoch 11 Average loss: 0.268126, Train accuracy: 0.951250, val accuracy: 0.650000\n",
      "epoch 12 Average loss: 0.235612, Train accuracy: 0.952500, val accuracy: 0.650000\n",
      "epoch 13 Average loss: 0.199025, Train accuracy: 0.966250, val accuracy: 0.655000\n",
      "epoch 14 Average loss: 0.182195, Train accuracy: 0.973750, val accuracy: 0.655000\n",
      "epoch 15 Average loss: 0.165799, Train accuracy: 0.971250, val accuracy: 0.700000\n",
      "epoch 16 Average loss: 0.135032, Train accuracy: 0.978750, val accuracy: 0.690000\n",
      "epoch 17 Average loss: 0.131463, Train accuracy: 0.982500, val accuracy: 0.680000\n",
      "epoch 18 Average loss: 0.123067, Train accuracy: 0.988750, val accuracy: 0.680000\n",
      "epoch 19 Average loss: 0.129687, Train accuracy: 0.980000, val accuracy: 0.670000\n",
      "epoch 20 Average loss: 0.109119, Train accuracy: 0.988750, val accuracy: 0.685000\n",
      "Compose(\n",
      "    ColorJitter(brightness=None, contrast=None, saturation=[0.8, 1.2], hue=[-0.2, 0.2])\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    RandomAdjustSharpness(sharpness_factor=3,p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.43, 0.44, 0.47], std=[0.2, 0.2, 0.2])\n",
      ")\n",
      "epoch 1 Average loss: 2.516191, Train accuracy: 0.151250, val accuracy: 0.195000\n",
      "epoch 2 Average loss: 2.193590, Train accuracy: 0.320000, val accuracy: 0.265000\n",
      "epoch 3 Average loss: 1.789145, Train accuracy: 0.442500, val accuracy: 0.385000\n",
      "epoch 4 Average loss: 1.438701, Train accuracy: 0.541250, val accuracy: 0.495000\n",
      "epoch 5 Average loss: 1.220891, Train accuracy: 0.660000, val accuracy: 0.535000\n",
      "epoch 6 Average loss: 0.971645, Train accuracy: 0.722500, val accuracy: 0.555000\n",
      "epoch 7 Average loss: 0.806916, Train accuracy: 0.772500, val accuracy: 0.610000\n",
      "epoch 8 Average loss: 0.650088, Train accuracy: 0.823750, val accuracy: 0.610000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [98], line 47\u001B[0m\n\u001B[0;32m     45\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(my_model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr, weight_decay\u001B[38;5;241m=\u001B[39mreg)\n\u001B[0;32m     46\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mStepLR(optimizer, step_size\u001B[38;5;241m=\u001B[39mstep_size, gamma\u001B[38;5;241m=\u001B[39mgamma)\n\u001B[1;32m---> 47\u001B[0m loss_history, train_history, val_history \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmy_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_num\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m hp \u001B[38;5;241m=\u001B[39m Hyperparams(transforms\u001B[38;5;241m=\u001B[39mtfs)\n\u001B[0;32m     49\u001B[0m rr \u001B[38;5;241m=\u001B[39m RunResult(val_history\u001B[38;5;241m=\u001B[39mval_history, final_val_accuracy\u001B[38;5;241m=\u001B[39mval_history[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "Cell \u001B[1;32mIn [55], line 12\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, val_loader, loss, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[0;32m     10\u001B[0m correct_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     11\u001B[0m total_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i_step, (x, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m     13\u001B[0m     prediction \u001B[38;5;241m=\u001B[39m model(x)\n\u001B[0;32m     14\u001B[0m     loss_value \u001B[38;5;241m=\u001B[39m loss(prediction, y)\n",
      "File \u001B[1;32m~\\Desktop\\dlcourse_ai\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\Desktop\\dlcourse_ai\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\Desktop\\dlcourse_ai\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Desktop\\dlcourse_ai\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Desktop\\dlcourse_ai\\venv\\lib\\site-packages\\torchvision\\datasets\\svhn.py:108\u001B[0m, in \u001B[0;36mSVHN.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    105\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(np\u001B[38;5;241m.\u001B[39mtranspose(img, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m)))\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 108\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    111\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32m~\\Desktop\\dlcourse_ai\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\Desktop\\dlcourse_ai\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Desktop\\dlcourse_ai\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1243\u001B[0m, in \u001B[0;36mColorJitter.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m   1235\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m   1236\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1237\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   1238\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Input image.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1241\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Color jittered image.\u001B[39;00m\n\u001B[0;32m   1242\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1243\u001B[0m     fn_idx, brightness_factor, contrast_factor, saturation_factor, hue_factor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_params\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1244\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbrightness\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontrast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaturation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhue\u001B[49m\n\u001B[0;32m   1245\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1247\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m fn_id \u001B[38;5;129;01min\u001B[39;00m fn_idx:\n\u001B[0;32m   1248\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m brightness_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Desktop\\dlcourse_ai\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1231\u001B[0m, in \u001B[0;36mColorJitter.get_params\u001B[1;34m(brightness, contrast, saturation, hue)\u001B[0m\n\u001B[0;32m   1229\u001B[0m c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m contrast \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(torch\u001B[38;5;241m.\u001B[39mempty(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39muniform_(contrast[\u001B[38;5;241m0\u001B[39m], contrast[\u001B[38;5;241m1\u001B[39m]))\n\u001B[0;32m   1230\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m saturation \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(torch\u001B[38;5;241m.\u001B[39mempty(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39muniform_(saturation[\u001B[38;5;241m0\u001B[39m], saturation[\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m-> 1231\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m hue \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muniform_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhue\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhue\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn_idx, b, c, s, h\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "epoch_num = 20\n",
    "loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "\n",
    "tfs = transforms.Compose([\n",
    "    transforms.ColorJitter(hue=.20, saturation=.20),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAdjustSharpness(3, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                       std=[0.20,0.20,0.20])\n",
    "])\n",
    "data_train = dset.SVHN('./data/', transform = tfs)\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "\n",
    "for i in range(10):\n",
    "    my_model = nn.Sequential(\n",
    "      nn.Conv2d(3, 6, 5, padding=0),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.BatchNorm2d(6),\n",
    "      nn.MaxPool2d(2),\n",
    "      nn.Conv2d(6, 16, 5, padding=0),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.BatchNorm2d(16),\n",
    "      nn.MaxPool2d(2),\n",
    "      Flattener(),\n",
    "      nn.Linear(16*5*5, 120),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.BatchNorm1d(120),\n",
    "      nn.Linear(120, 84),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.BatchNorm1d(84),\n",
    "      nn.Linear(84, 10)\n",
    "      )\n",
    "\n",
    "    #lr = 10**np.random.uniform(-5, -1)\n",
    "    lr = 0.0061\n",
    "    reg = 3.4e-5\n",
    "    gamma = 0.5\n",
    "    step_size = 4\n",
    "    print(tfs)\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(my_model.parameters(), lr=lr, weight_decay=reg)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    loss_history, train_history, val_history = train_model(my_model, train_loader, val_loader, loss, optimizer, scheduler, epoch_num)\n",
    "    hp = Hyperparams(transforms=tfs)\n",
    "    rr = RunResult(val_history=val_history, final_val_accuracy=val_history[-1])\n",
    "    run_record[hp] = rr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = sorted(list(run_record.items()), key=lambda x: x[1].final_val_accuracy)[::-1]\n",
    "for a, b in result:\n",
    "    print(a, b.final_val_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6808\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=batch_size)\n",
    "test_accuracy = compute_accuracy(my_model, test_loader)\n",
    "print(\"Test accuracy: %2.4f\" % test_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
